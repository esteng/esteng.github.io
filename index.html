<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Elias Stengel-Eskin</title> <meta name="author" content="Elias Stengel-Eskin"/> <meta name="description" content="Postdoctoral Research Associate, UNC Chapel Hill "/> <meta name="keywords" content="NLP, AI, Computational Linguistics"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>/assest/img/icon.png</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://esteng.github.io/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/_pages/cv/">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/talks/">talks</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> Elias Stengel-Eskin </h1> <p class="desc"></p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/headshot_new-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 900px)" srcset="/assets/img/headshot_new-900.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/headshot_new-1400.webp"></source> <img src="/assets/img/headshot_new.jpg" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="headshot_new.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="address"> </div> </div> <div class="clearfix"> <p><strong>I will be recruiting Ph.D. students for Fall 2026 and also looking for interns throughout 2025-2026, please see this <a href="/contact/">page for more details</a>.</strong></p> <p>I am an Assistant Professor of Computer Science at The University of Texas at Austin where I work on developing AI agents that can intelligently communicate and collaborate with people and each other. My work addresses three key problems:</p> <ol> <li>A central focus of my work is <strong>multi-agent communication and collaboration</strong>, which has led to work on multi-LLM multi-round discussions/debates, distilling multi-agent behavior, pragmatic/verbalized uncertainty, and persuasion.</li> <li>Agents must be <strong>grounded to the world through their inputs and actions</strong>: another line of my work covers multimodal grounding and converting language to action through semantic parsing, text-to-code, and learning abstractions and skills.</li> <li>Developing safe and robust agents means <strong>handling uncertainty, ambiguity, and underspecification</strong>. As we scale up tasks, underspecification and ambiguity will become increasingly relevant, especially when predicting actions/grounding to the world. My work covers calibration and uncertainty especially in connection with implicit phenomena such as vagueness, underspecification, and ambiguity. While I’ve mostly explored these topics through a linguistic lens, I am interested in their importance to intelligence more broadly.</li> </ol> <p>Concretely, some of the main areas I’ve been publishing on recently are:</p> <ul> <li> <strong>Confidence Estimation and Calibration</strong>: <ul> <li><a href="https://arxiv.org/abs/2405.21028" target="_blank" rel="noopener noreferrer">on multi-agent pragmatic reasoning for better calibration (Stengel-Eskin et al., NeurIPS 2024)</a></li> <li><a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00598/117737" target="_blank" rel="noopener noreferrer">on calibration in semantic parsing (Stengel-Eskin and Van Durme, TACL 2023)</a></li> <li><a href="https://aclanthology.org/2023.emnlp-main.159/" target="_blank" rel="noopener noreferrer">on balancing safety and usability using confidence (Stengel-Eskin and Van Durme, EMNLP 2023)</a></li> </ul> </li> <li> <strong>Ambiguity and Underspecification</strong>: <ul> <li><a href="https://arxiv.org/abs/2508.19546" target="_blank" rel="noopener noreferrer">on exploitation of loopholes instatiated by ambiguity (Choi, Bansal, and Stengel-Eskin, EMNLP 2025)</a></li> <li><a href="https://openreview.net/forum?id=qLegogRepu" target="_blank" rel="noopener noreferrer">on ambiguity in semantic parsing (Stengel-Eskin and Van Durme, ICLR 2024)</a></li> <li><a href="https://openreview.net/forum?id=L4nOxziGf9" target="_blank" rel="noopener noreferrer">on improving VQA through preemptive self-clarification (Prasad et al., ICLR 2024)</a></li> <li><a href="https://aclanthology.org/2023.acl-long.569/" target="_blank" rel="noopener noreferrer">on rephrasing and analyzing ambiguous questions in VQA (Stengel-Eskin et al. ACL 2023)</a></li> </ul> </li> <li> <strong>Multi-Agent/Multi-Model Reasoning</strong>: <ul> <li><a href="https://arxiv.org/abs/2410.14596" target="_blank" rel="noopener noreferrer">on multi-agent training to accept good and resist bad persuasion (Stengel-Eskin et al., NAACL 2025)</a></li> <li><a href="https://arxiv.org/abs/2402.01620" target="_blank" rel="noopener noreferrer">on structured distillation to learn from multi-agent discussions (Chen et al., ICML 2024)</a></li> <li><a href="https://arxiv.org/abs/2402.12348" target="_blank" rel="noopener noreferrer">on a new benchmark to assess game-theoretic abilities for LLM agents (Duan et al., NeurIPS 2024)</a></li> <li><a href="https://arxiv.org/abs/2409.12147" target="_blank" rel="noopener noreferrer">on multi-agent iterative coarse-to-fine refinement for reasoning tasks (Chen et al., 2024)</a></li> <li><a href="https://arxiv.org/abs/2410.01735" target="_blank" rel="noopener noreferrer">on using bandits to select instance-level reward models for LLM alignment (Nguyen et al., 2024)</a></li> <li><a href="https://arxiv.org/abs/2503.05641" target="_blank" rel="noopener noreferrer">on combining 16 LLMs on 1 GPU via symbolic MoE (Chen et al., 2025)</a></li> </ul> </li> <li> <strong>Learning Skills and Abstractions for Agents/Coding/Planning</strong>: <ul> <li><a href="https://arxiv.org/abs/2401.16467" target="_blank" rel="noopener noreferrer">on learning coding abstractions with LLMs (Stengel-Eskin et al., ICML 2024)</a></li> <li><a href="https://arxiv.org/abs/2402.16354" target="_blank" rel="noopener noreferrer">on discovering skills for RL agents (Fu et al., ICML 2024)</a></li> <li><a href="https://arxiv.org/abs/2405.02749" target="_blank" rel="noopener noreferrer">on distilling skills from teachers for LLM agents (Hashemzadeh et al., CoLLAs 2024)</a></li> <li><a href="https://arxiv.org/abs/2407.14414" target="_blank" rel="noopener noreferrer">on hybrid and controllable System 1 and System 2 planning (Saha et al., ICLR 2025)</a></li> <li><a href="https://arxiv.org/abs/2502.01619" target="_blank" rel="noopener noreferrer">on learning to generate unit tests for LLM debugging (Prasad et al., 2025)</a></li> <li><a href="https://zaidkhan.me/EFAGen/" target="_blank" rel="noopener noreferrer">on inferring structured abstractions for reasoning (Khan et al., 2025)</a></li> </ul> </li> <li> <strong>Improving Multimodal Models and LLM Agents</strong>: <ul> <li><a href="https://dataenvgym.github.io" target="_blank" rel="noopener noreferrer">on building and testing data generation agents for creating training data (Khan et al., ICLR 2025 Spotlight)</a></li> <li><a href="https://arxiv.org/abs/2405.19209" target="_blank" rel="noopener noreferrer">on a tree-based representation for LLM-based video reasoning (Wang et al. CVPR 2025)</a></li> <li><a href="https://arxiv.org/abs/2403.02325" target="_blank" rel="noopener noreferrer">on improving visual prompting/object grounding without training (Wan et al., ECCV 2024)</a></li> <li><a href="https://arxiv.org/abs/2402.13212" target="_blank" rel="noopener noreferrer">on a more effective/efficient self-consistency method for LLM agents (Wang et al., ACL 2024)</a></li> <li><a href="https://arxiv.org/abs/2406.11665" target="_blank" rel="noopener noreferrer">on Western cultural bias in VLMs and the effect of pretraining language (Ananthram et al., ICLR 2025)</a></li> <li><a href="https://aclanthology.org/2022.naacl-main.390/" target="_blank" rel="noopener noreferrer">on visual commonsense in unimodal and multimodal models (Zhang et al, NAACL 2022)</a></li> <li><a href="https://arxiv.org/abs/2504.15485" target="_blank" rel="noopener noreferrer">on reasoning about occluded patterns (Pothiraj et al., 2025)</a></li> </ul> </li> </ul> <p>Previously, I was a postdoc with <a href="https://www.cs.unc.edu/~mbansal/" target="_blank" rel="noopener noreferrer">Mohit Bansal</a> at UNC Chapel Hill. I received my Ph.D. in 2023 from Johns Hopkins University, where I was supervised by <a href="https://www.cs.jhu.edu/~vandurme/" target="_blank" rel="noopener noreferrer">Benjamin Van Durme</a> and supported by an NSF GRFP. Before starting my Ph.D., I received my B.A.&amp;Sc. with First Class Honours in Cognitive Science from McGill University, focusing in computer science and linguistics. While at McGill, I worked as a research assistant at the Montreal Language Modeling Lab (MLML), now <a href="https://mcqll.org" target="_blank" rel="noopener noreferrer">MCQLL</a> supervised by <a href="http://people.linguistics.mcgill.ca/~morgan/" target="_blank" rel="noopener noreferrer">Morgan Sonderegger</a>. I wrote my honours thesis (supervised by <a href="https://todonnell.github.io" target="_blank" rel="noopener noreferrer">Timothy O’Donnell</a>) on a variational inference algorithm for a model of language acquisition.</p> </div> <div class="news"> <h2>news</h2> <div class="table-responsive" style="max-height: 20vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row">Jul 19, 2025</th> <td> I’ll be at ACL helping to present two papers: <a href="https://arxiv.org/abs/2506.01187" target="_blank" rel="noopener noreferrer">LAQuer: Localized Attribution Queries in Content-grounded Generation</a> and <a href="https://arxiv.org/abs/2502.12446" target="_blank" rel="noopener noreferrer">Multi-Attribute Steering of Language Models via Targeted Intervention</a>! </td> </tr> <tr> <th scope="row">Jul 8, 2025</th> <td> Excited to announce 4 papers accepted to COLM! <a href="https://arxiv.org/abs/2502.01619" target="_blank" rel="noopener noreferrer">Learning to Generate Unit Tests for Automated Debugging</a>, <a href="https://arxiv.org/abs/2504.07389" target="_blank" rel="noopener noreferrer">Task-Circuit Quantization: Leveraging Knowledge Localization and Interpretability for Compression</a>, <a href="https://arxiv.org/abs/2506.14580" target="_blank" rel="noopener noreferrer">GenerationPrograms: Fine-grained Attribution with Executable Programs</a>, and <a href="https://arxiv.org/abs/2504.13079" target="_blank" rel="noopener noreferrer">Retrieval-Augmented Generation with Conflicting Evidence</a>! </td> </tr> <tr> <th scope="row">May 5, 2025</th> <td> Extremely excited to <a href="https://x.com/EliasEskin/status/1919457159806439685" target="_blank" rel="noopener noreferrer">announce</a> that I will be joining UT Austin as an Assistant Professor in Computer Science in Fall 2025! </td> </tr> <tr> <th scope="row">Feb 26, 2025</th> <td> <a href="https://videotree2024.github.io" target="_blank" rel="noopener noreferrer">VideoTree: Adaptive Tree-based Video Representation for LLM Reasoning on Long Videos</a> accepted to CVPR 2025! VideoTree augments LLMs with tree structure for reasoning over long videos. </td> </tr> <tr> <th scope="row">Feb 11, 2025</th> <td> <a href="https://dataenvgym.github.io" target="_blank" rel="noopener noreferrer">DataEnvGym: Data Generation Agents in Teacher Environments with Student Feedback</a> led by <a href="https://zaidkhan.me" target="_blank" rel="noopener noreferrer">Zaid Khan</a> with <a href="https://j-min.io" target="_blank" rel="noopener noreferrer">Jaemin Cho</a> and <a href="https://www.cs.unc.edu/~mbansal" target="_blank" rel="noopener noreferrer">Mohit Bansal</a> has been selected as a ICLR 2025 Spotlight! </td> </tr> <tr> <th scope="row">Feb 7, 2025</th> <td> New preprint! <a href="https://arxiv.org/abs/2502.01619" target="_blank" rel="noopener noreferrer">Learning to Generate Unit Tests for Automated Debugging</a> with <a href="https://archiki.github.io" target="_blank" rel="noopener noreferrer">Archiki Prasad</a>, <a href="https://dinobby.github.io" target="_blank" rel="noopener noreferrer">Justin Chih-Yao Chen</a>, <a href="https://zaidkhan.me" target="_blank" rel="noopener noreferrer">Zaid Khan</a>, and <a href="https://www.cs.unc.edu/~mbansal" target="_blank" rel="noopener noreferrer">Mohit Bansal</a> on how to improve automated debugging with generated unit tests. We train unit test generators, leveraging test-time scaling and validation across tests to improve debugging! </td> </tr> <tr> <th scope="row">Jan 23, 2025</th> <td> Six new papers accepted, three to ICLR and three to NAACL! <a href="https://dataenvgym.github.io" target="_blank" rel="noopener noreferrer">DataEnvGym</a> (ICLR) introduces a new framework for developing agents that adaptively generate data for training student models. <a href="https://arxiv.org/abs/2407.14414" target="_blank" rel="noopener noreferrer">System 1.x</a> (ICLR): planning with LLMs that balances quick action prediction with slower/more deliberate planning through verbalizing search traces. <a href="https://openreview.net/forum?id=Xbl6t6zxZs" target="_blank" rel="noopener noreferrer">See It from My Perspective</a> (ICLR) quantifies the effect of language on cultural bias in large vision-language models. <a href="https://arxiv.org/abs/2410.14596" target="_blank" rel="noopener noreferrer">Persuasion-Balanced Training</a> (NAACL): multi-agent training method teaching models to balance accepting good persuasion while resisting misinformation/bad persuasion. <a href="https://arxiv.org/abs/2409.07394" target="_blank" rel="noopener noreferrer">AdaCAD</a> (NAACL): an adaptive method for balancing retrieved/context knowledge with a model’s parametric knowledge. <a href="https://openreview.net/forum?id=HSkKSjSRCe#discussion" target="_blank" rel="noopener noreferrer">MAMM-Refine</a> (NAACL) improves generation through multi-agent multi-model discussion, focusing on refinement. </td> </tr> <tr> <th scope="row">Nov 6, 2024</th> <td> Our philosophy collaboration on challenges in model editing was accepted to TMLR! <a href="https://arxiv.org/abs/2406.19354" target="_blank" rel="noopener noreferrer">Fundamental Problems With Model Editing: How Should Rational Belief Revision Work in LLMs?</a> </td> </tr> <tr> <th scope="row">Oct 21, 2024</th> <td> New paper out! <a href="https://arxiv.org/abs/2410.14596" target="_blank" rel="noopener noreferrer">Teaching Models to Balance Resisting and Accepting Persuasion</a>, where we use multi-agent recursive dialogue trees to teach models to accept and resist persuasion <em>when appropriate</em>. Our method reduces susceptibility to misinformation and flipflopping while also improving LLMs’ ability to act together in a team thru multi-agent dialogue! </td> </tr> <tr> <th scope="row">Oct 11, 2024</th> <td> New preprint! <a href="https://dataenvgym.github.io" target="_blank" rel="noopener noreferrer">DataEnvGym: Data Generation Agents in Teacher Environments with Student Feedback</a> led by <a href="https://zaidkhan.me" target="_blank" rel="noopener noreferrer">Zaid Khan</a> with <a href="https://j-min.io" target="_blank" rel="noopener noreferrer">Jaemin Cho</a> and <a href="https://www.cs.unc.edu/~mbansal" target="_blank" rel="noopener noreferrer">Mohit Bansal</a> on a novel testbed for creating data generation agents. These agents produce synthetic data for teaching student models based on their errors and weaknesses! </td> </tr> <tr> <th scope="row">Oct 3, 2024</th> <td> New preprint! <a href="https://arxiv.org/abs/2410.01735" target="_blank" rel="noopener noreferrer">LASeR: Learning to Adaptively Select Reward Models with Multi-Armed Bandits</a> led by <a href="https://duykhuongnguyen.github.io" target="_blank" rel="noopener noreferrer">Duy Nguyen</a> and <a href="https://archiki.github.io" target="_blank" rel="noopener noreferrer">Archiki Prasad</a> with <a href="https://www.cs.unc.edu/~mbansal/" target="_blank" rel="noopener noreferrer">Mohit Bansal</a> on using bandit methods to pick the best-suited RM to optimize at an instance level, improving LLMs on reasoning, instruction-following, and long-context understanding. </td> </tr> <tr> <th scope="row">Oct 2, 2024</th> <td> Two papers accepted to NeurIPS 2024! <a href="https://arxiv.org/abs/2405.21028" target="_blank" rel="noopener noreferrer">LACIE: Listener-Aware Finetuning for Confidence Calibration in Large Language Models</a> uses pragmatics to calibrate LLMs, and <a href="https://arxiv.org/abs/2402.12348" target="_blank" rel="noopener noreferrer">GTBench: Uncovering the Strategic Reasoning Limitations of LLMs via Game-Theoretic Evaluations</a> introduces a new game-theoretic benchmark. </td> </tr> <tr> <th scope="row">Sep 19, 2024</th> <td> New preprint! <a href="https://arxiv.org/abs/2409.12147" target="_blank" rel="noopener noreferrer">MAgICoRe: Multi-Agent, Iterative, Coarse-to-Fine Refinement for Reasoning</a> led by <a href="https://dinobby.github.io" target="_blank" rel="noopener noreferrer">Justin Chih-Yao Chen</a> with <a href="https://swarnahub.github.io" target="_blank" rel="noopener noreferrer">Swarnadeep Saha</a>, <a href="https://archiki.github.io" target="_blank" rel="noopener noreferrer">Archiki Prasad</a>, and <a href="https://www.cs.unc.edu/~mbansal/" target="_blank" rel="noopener noreferrer">Mohit Bansal</a> introduces a novel method for refinement that improves math reasoning by selectively refining only hard instances and by treating it as an iterative, multi-agent problem. </td> </tr> <tr> <th scope="row">Sep 14, 2024</th> <td> New preprint! <a href="https://arxiv.org/abs/2409.07394" target="_blank" rel="noopener noreferrer">AdaCAD: Adaptively Decoding to Balance Conflicts between Contextual and Parametric Knowledge</a> led by <a href="https://hannight.github.io" target="_blank" rel="noopener noreferrer">Han Wang</a> with <a href="https://archiki.github.io" target="_blank" rel="noopener noreferrer">Archiki Prasad</a> and <a href="https://www.cs.unc.edu/~mbansal/" target="_blank" rel="noopener noreferrer">Mohit Bansal</a> introduces a dynamic decoding strategy to deal with variable amounts of knowledge conflict. </td> </tr> <tr> <th scope="row">Jul 1, 2024</th> <td> <a href="https://arxiv.org/abs/2403.02325" target="_blank" rel="noopener noreferrer">Contrastive Region Guidance: Improving Grounding in Vision-Language Models without Training</a> has been accepted to ECCV 2024! </td> </tr> <tr> <th scope="row">Jun 3, 2024</th> <td> New preprint! <a href="https://arxiv.org/abs/2405.21028" target="_blank" rel="noopener noreferrer">LACIE: Listener-Aware Finetuning for Confidence Calibration in Large Language Models</a> tackles implicit and explicit calibration in LLMs by using insights from pragmatics! </td> </tr> <tr> <th scope="row">May 28, 2024</th> <td> New project on videos+LLMs! <a href="https://videotree2024.github.io" target="_blank" rel="noopener noreferrer">VideoTree: Adaptive Tree-based Video Representation for LLM Reasoning on Long Videos</a> uses a tree-based structure to help LLMs reason over long videos efficiently and effectively. Joint work with <a href="https://ziyangw2000.github.io" target="_blank" rel="noopener noreferrer">Ziyang Wang</a> and <a href="https://yui010206.github.io" target="_blank" rel="noopener noreferrer">Shoubin Yu</a>. </td> </tr> <tr> <th scope="row">May 15, 2024</th> <td> <a href="https://arxiv.org/abs/2402.13212" target="_blank" rel="noopener noreferrer">Soft Self-Consistency Improves Language Model Agents</a> has been accepted to ACL 2024! </td> </tr> <tr> <th scope="row">May 4, 2024</th> <td> Three papers accepted to ICML 2024! <a href="https://arxiv.org/abs/2401.16467" target="_blank" rel="noopener noreferrer">ReGAL: Refactoring Programs to Discover Generalizable Abstractions </a>, which uses refactoring to discover program abstractions for LLM-based code generation, <a href="https://arxiv.org/abs/2402.01620" target="_blank" rel="noopener noreferrer">MAGDi: Structured Distillation of Multi-Agent Interaction Graphs Improves Reasoning in Smaller Language Models </a>, which introduces a structured distillation method for learning from discussions between multiple LLMs, and <a href="https://arxiv.org/abs/2402.16354" target="_blank" rel="noopener noreferrer">Language-guided Skill Learning with Temporal Variational Inference</a>, which learns reusable skills from trajectories of demonstrations. </td> </tr> <tr> <th scope="row">Mar 22, 2024</th> <td> Excited to be giving a keynote at the <a href="https://uncertainlp.github.io" target="_blank" rel="noopener noreferrer">UncertaiNLP workshop</a> at EACL 2024, titled Confidence-based Rephrasing, Refinement, and Selection. I’ll cover a wide range of topics including <a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00598/117737" target="_blank" rel="noopener noreferrer">calibration in semantic parsing</a>, <a href="https://arxiv.org/abs/2303.16857" target="_blank" rel="noopener noreferrer">using calibrated models to improve usability</a>, <a href="https://arxiv.org/abs/2310.05861" target="_blank" rel="noopener noreferrer">underspecified visual question answering</a> and much more! </td> </tr> <tr> <th scope="row">Mar 5, 2024</th> <td> New work with <a href="https://meetdavidwan.github.io" target="_blank" rel="noopener noreferrer">David Wan</a> and <a href="https://j-min.io" target="_blank" rel="noopener noreferrer">Jaemin Cho</a> on improving visual tasks (especially grounding) through region-based guidance in <a href="https://arxiv.org/abs/2403.02325" target="_blank" rel="noopener noreferrer">Contrastive Region Guidance: Improving Grounding in Vision-Language Models without Training</a> </td> </tr> <tr> <th scope="row">Feb 3, 2024</th> <td> New work led by <a href="https://dinobby.github.io" target="_blank" rel="noopener noreferrer">Justin Chen</a> and <a href="https://swarnahub.github.io" target="_blank" rel="noopener noreferrer">Swarnadeep Saha</a> on distilling multi-agent LLM interactions into smaller models: <a href="https://arxiv.org/abs/2402.01620" target="_blank" rel="noopener noreferrer">MAGDi: Structured Distillation of Multi-Agent Interaction Graphs Improves Reasoning in Smaller Language Models </a>. MAGDi uses a graph structure on top of LLM dialogues to distill reasoning from several large teacher models into a single, lightweight student. </td> </tr> <tr> <th scope="row">Jan 30, 2024</th> <td> New preprint! <a href="https://arxiv.org/abs/2401.16467" target="_blank" rel="noopener noreferrer">ReGAL: Refactoring Programs to Discover Generalizable Abstractions </a> introduces a new refactoring-based method for learning abstractions for LLM program prediction, improving performance on a variety of tasks. Joint work with <a href="https://archiki.github.io" target="_blank" rel="noopener noreferrer">Archiki Prasad</a> as part of my postdoc at UNC. </td> </tr> <tr> <th scope="row">Jan 17, 2024</th> <td> Two papers accepted to ICLR 2024. <a href="https://arxiv.org/abs/2306.00824" target="_blank" rel="noopener noreferrer">Zero and Few-shot Semantic Parsing with Ambiguous Inputs</a> introduces a new benchmark for semantic parsing with ambiguity and tests a variety of models on how they handle five common linguistic ambiguities. <a href="https://arxiv.org/abs/2310.05861" target="_blank" rel="noopener noreferrer">Rephrase, Augment, Reason: Visual Grounding of Questions for Vision-Language Models </a> is the first paper from my new postdoc position and introduces RepARe, a method for augmenting and rephrasing VQA questions (especially underspecified ones) to make them easier for zero-shot VL models to answer. </td> </tr> <tr> <th scope="row">Jan 16, 2024</th> <td> My thesis is now publicly available: <a href="https://jscholarship.library.jhu.edu/items/55f80bf4-7c24-4e27-b158-3fa6b8ce1da8" target="_blank" rel="noopener noreferrer">Modeling Meaning for Description and Interaction</a>. Many thanks to my advisor <a href="https://www.cs.jhu.edu/~vandurme/" target="_blank" rel="noopener noreferrer">Benjamin Van Durme</a> for all of your guidance over the last five years and to my thesis committee <a href="https://web.mit.edu/jda/www/" target="_blank" rel="noopener noreferrer">Jacob Andreas</a> and <a href="https://rawlins.io" target="_blank" rel="noopener noreferrer">Kyle Rawlins</a> for your feedback! </td> </tr> <tr> <th scope="row">Jun 3, 2023</th> <td> I’m incredibly excited to announce that I will be starting a Postdoc with <a href="http://www.cs.unc.edu/~mbansal/" target="_blank" rel="noopener noreferrer">Mohit Bansal</a> at the University of North Carolina, Chapel Hill! Looking forward to lots of collaborations with the amazing students and faculty of <a href="https://nlp.cs.unc.edu" target="_blank" rel="noopener noreferrer">UNC NLP</a> and <a href="https://cs.unc.edu" target="_blank" rel="noopener noreferrer">UNC CS</a>! </td> </tr> <tr> <th scope="row">Jun 1, 2023</th> <td> <a href="https://arxiv.org/abs/2211.07443" target="_blank" rel="noopener noreferrer">Calibrated Interpretation: Confidence Estimation in Semantic Parsing</a> has just been accepted to TACL! We examine the calibration of common semantic parsing models, including LLMs using in-context learning. Check out the paper for results across a number of tasks and datasets! </td> </tr> <tr> <th scope="row">May 3, 2023</th> <td> <a href="https://arxiv.org/abs/2211.07516" target="_blank" rel="noopener noreferrer">Why Did the Chicken Cross the Road? Rephrasing and Analyzing Ambiguous Questions in VQA</a> has been accepted to ACL 2023! We introduce a brand new dataset of ambiguous questions in VQA, with a model disambiguation model and plenty of linguistic analysis. See you in Toronto! </td> </tr> <tr> <th scope="row">Mar 31, 2023</th> <td> I’ve restructured a previous pre-print into two different papers. <a href="https://arxiv.org/abs/2211.07443" target="_blank" rel="noopener noreferrer">The first</a> focuses on cataloguing calibration in popular semantic parsing systems, and the <a href="https://arxiv.org/abs/2303.16857" target="_blank" rel="noopener noreferrer">second</a> looks at what we can do with a well-calibrated model. </td> </tr> <tr> <th scope="row">Feb 28, 2023</th> <td> Super-CLEVR (<span style="color:#84639c">CVPR highlight</span>), an exciting new benchmark for generalization in vision tasks led by <a href="https://lizw14.github.io" target="_blank" rel="noopener noreferrer">Zhuowan Li</a> now accepted to CVPR 2023 as a highlight (~2% of submissions)! <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Super-CLEVR_A_Virtual_Benchmark_To_Diagnose_Domain_Robustness_in_Visual_CVPR_2023_paper.pdf" target="_blank" rel="noopener noreferrer">Super-CLEVR: A Virtual Benchmark to Diagnose Domain Robustness in Visual Reasoning</a> </td> </tr> </table> </div> </div> <div class="social"> <div class="contact-icons"> <a href="https://scholar.google.com/citations?user=gr_ZVSQAAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/esteng" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a> <a href="https://twitter.com/EliasEskin" title="Twitter" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter"></i></a> </div> <div class="contact-note"> Reach me by email: esteng [at] cs [dot] unc [dot] edu </div> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Elias Stengel-Eskin. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>