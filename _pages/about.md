---
layout: about
title: about
permalink: /

profile:
  align: right
  image: headshot_new.jpg
  image_circular: false # crops the image to make it circular
  address: >

news: true  # includes a list of news items
selected_papers: false # includes a list of papers marked as "selected={true}"
social: true  # includes social icons at the bottom of the page
---

I am a Postdoctoral Research Associate at the University of North Carolina, Chapel Hill in the [MURGe-Lab](https://murgelab.cs.unc.edu) led by [Mohit Bansal](https://murgelab.cs.unc.edu). 
I received my Ph.D. in 2023 from Johns Hopkins University, where I was supervised by [Benjamin Van Durme](https://www.cs.jhu.edu/~vandurme/) and supported by an NSF GRFP. 

I aim to develop AI agents that can intelligently communicate and collaborate with people and each other.
My work addresses three key problems:
1. A central focus of my work is **multi-agent communication and collaboration**, which has led to work on multi-LLM multi-round discussions/debates, distilling multi-agent behavior, pragmatic/verbalized uncertainty, and persuasion. 
2. Agents must be **grounded to the world through their inputs and actions**: another line of my work covers multimodal grounding and converting language to action through semantic parsing, text-to-code, and learning abstractions and skills. 
3. Developing safe and robust agents means **handling uncertainty, ambiguity, and underspecification**. As we scale up tasks, underspecification and ambiguity will become increasingly relevant, especially when predicting actions/grounding to the world.
My work covers calibration and uncertainty especially in connection with implicit phenomena such as vagueness, underspecification, and ambiguity. 
While I've mostly explored these topics through a linguistic lens, I am interested in their importance to intelligence more broadly.

Concretely, some of the areas I've been publishing on recently are:

- **Confidence Estimation and Calibration**: 
    - [on multi-agent pragmatic reasoning for better calibration (Stengel-Eskin et al., NeurIPS 2024)](https://arxiv.org/abs/2405.21028)
    - [on calibration in semantic parsing (Stengel-Eskin and Van Durme, TACL 2023)](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00598/117737)
    - [on balancing safety and usability using confidence (Stengel-Eskin and Van Durme, EMNLP 2023)](https://aclanthology.org/2023.emnlp-main.159/)

- **Ambiguity and Underspecification**: 
    - [on ambiguity in semantic parsing (Stengel-Eskin and Van Durme, ICLR 2024)](https://openreview.net/forum?id=qL9gogRepu) 
    - [on improving VQA through preemptive self-clarification (Prasad et al., ICLR 2024)](https://openreview.net/forum?id=L4nOxziGf9)
    - [on rephrasing and analyzing ambiguous questions in VQA (Stengel-Eskin et al. ACL 2023)](https://aclanthology.org/2023.acl-long.569/) 

- **Multi-Agent/Multi-Model Reasoning**: 
    - [on multi-agent training to accept good and resist bad persuasion (Stengel-Eskin et al., 2024)](https://arxiv.org/abs/2410.14596) 
    - [on structured distillation to learn from multi-agent discussions (Chen et al., ICML 2024)](https://arxiv.org/abs/2402.01620)
    - [on a new benchmark to assess game-theoretic abilities for LLM agents (Duan et al., NeurIPS 2024)](https://arxiv.org/abs/2402.12348)
    - [on multi-agent iterative coarse-to-fine refinement for reasoning tasks (Chen et al., 2024)](https://arxiv.org/abs/2409.12147) 
    - [on using bandits to select instance-level reward models for LLM alignment (Nguyen et al., 2024)](https://arxiv.org/abs/2410.01735) 

- **Learning Skills and Abstractions for Agents/Coding/Planning**: 
    - [on learning coding abstractions with LLMs (Stengel-Eskin et al., ICML 2024)](https://arxiv.org/abs/2401.16467)
    - [on discovering skills for RL agents (Fu et al., ICML 2024)](https://arxiv.org/abs/2402.16354)
    - [on distilling skills from teachers for LLM agents (Hashemzadeh et al., CoLLAs 2024)](https://arxiv.org/abs/2405.02749)
    - [on hybrid and controllable System 1 and System 2 planning (Saha et al., 2024)](https://arxiv.org/abs/2407.14414) 


- **Improving Multimodal Models and LLM Agents**:
    - [on building and testing data generation agents for creating training data (Khan et al., 2024](https://dataenvgym.github.io) 
    - [on a tree-based representation for LLM-based video reasoning (Wang et al. 2024)](https://arxiv.org/abs/2405.19209)
    - [on improving visual prompting/object grounding without training (Wan et al., ECCV 2024)](https://arxiv.org/abs/2403.02325) 
    - [on a more effective/efficient self-consistency method for LLM agents (Wang et al., ACL 2024)](https://arxiv.org/abs/2402.13212)
    - [on Western cultural bias in VLMs and the effect of pretraining language (Ananthram et al., 2024)](https://arxiv.org/abs/2406.11665) 
    - [on visual commonsense in unimodal and multimodal models (Zhang et al, 2022)](https://aclanthology.org/2022.naacl-main.390/) 


Before starting my Ph.D., I received my B.A.&Sc. with First Class Honours in Cognitive Science from McGill University, focusing in computer science and linguistics.
While at McGill, I worked as a research assistant at the Montreal Language Modeling Lab (MLML), now [MCQLL](https://mcqll.org) supervised by [Morgan Sonderegger](http://people.linguistics.mcgill.ca/~morgan/).
I wrote my honours thesis (supervised by [Timothy O'Donnell](https://todonnell.github.io)) on a variational inference algorithm for a model of language acquisition.

